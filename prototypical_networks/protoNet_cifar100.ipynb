{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "protoNet_cifar100.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazzad-hasan/few-shot-learning/blob/main/prototypical_networks/protoNet_cifar100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZeA3KjVP22l"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/shazzad-hasan/few-shot-learning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "LeVg4ELpK0Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/few-shot-learning/prototypical_networks"
      ],
      "metadata": {
        "id": "vA9zo9SsK0UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "T7Tu2vjaK0Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
      ],
      "metadata": {
        "id": "we-s29qtWFXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split \n",
        "from torch.utils.data.sampler import SubsetRandomSampler \n",
        "\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "EtB-iB1vK1Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import local helper functions\n",
        "from helper_dataset import ImageDataset, FewShotBatchSampler\n",
        "from helper_train import train\n",
        "from helper_evaluate import test"
      ],
      "metadata": {
        "id": "rdLQ4rSuNCAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if cuda is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "  print(\"CUDA is available!\")\n",
        "else:\n",
        "  print(\"CUDA is not available\")\n",
        "\n",
        "device = torch.device('cuda') if train_on_gpu else torch.device('cpu')"
      ],
      "metadata": {
        "id": "CxmLb24nNDDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BMmTtf4qNDG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_train_set = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "CIFAR_test_set = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "KNKxzvSyLgZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 12\n",
        "CIFAR_images = torch.stack([CIFAR_train_set[np.random.randint(len(CIFAR_train_set))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the CIFAR100 dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "wz3nn9-WLgcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_all_images = np.concatenate([CIFAR_train_set.data, CIFAR_test_set.data], axis=0)\n",
        "CIFAR_all_targets = torch.LongTensor(CIFAR_train_set.targets + CIFAR_test_set.targets)"
      ],
      "metadata": {
        "id": "3GBPU6oELgfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)           # Set seed for reproducibility\n",
        "classes = torch.randperm(100)  # Returns random permutation of numbers 0 to 99\n",
        "train_classes, val_classes, test_classes = classes[:80], classes[80:90], classes[90:]"
      ],
      "metadata": {
        "id": "IIBjpJQYLgi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_from_labels(imgs, targets, class_set, **kwargs):\n",
        "    class_mask = (targets[:,None] == class_set[None,:]).any(dim=-1)\n",
        "    return ImageDataset(imgs=imgs[class_mask],\n",
        "                        targets=targets[class_mask],\n",
        "                        **kwargs)"
      ],
      "metadata": {
        "id": "dcBJrxFIMUOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_MEANS = torch.Tensor([0.5183975 , 0.49192241, 0.44651328])\n",
        "DATA_STD = torch.Tensor([0.26770132, 0.25828985, 0.27961241])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(\n",
        "                                         DATA_MEANS, DATA_STD)\n",
        "                                     ])\n",
        "# For training, we add some augmentation.\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop(\n",
        "                                          (32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(\n",
        "                                          DATA_MEANS, DATA_STD)\n",
        "                                      ])\n",
        "\n",
        "train_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, train_classes, img_transform=train_transform)\n",
        "val_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, val_classes, img_transform=test_transform)\n",
        "test_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, test_classes, img_transform=test_transform)"
      ],
      "metadata": {
        "id": "Xxms5AJIMURf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_WAY = 5\n",
        "K_SHOT = 4\n",
        "train_data_loader = DataLoader(train_set,\n",
        "                                    batch_sampler=FewShotBatchSampler(train_set.targets,\n",
        "                                                                      include_query=True,\n",
        "                                                                      N_way=N_WAY,\n",
        "                                                                      K_shot=K_SHOT,\n",
        "                                                                      shuffle=True),\n",
        "                                    num_workers=4)\n",
        "val_data_loader = DataLoader(val_set,\n",
        "                                  batch_sampler=FewShotBatchSampler(val_set.targets,\n",
        "                                                                    include_query=True,\n",
        "                                                                    N_way=N_WAY,\n",
        "                                                                    K_shot=K_SHOT,\n",
        "                                                                    shuffle=False,\n",
        "                                                                    shuffle_once=True),\n",
        "                                  num_workers=4)"
      ],
      "metadata": {
        "id": "4CBdsb46MUUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_batch(imgs, targets):\n",
        "    support_imgs, query_imgs = imgs.chunk(2, dim=0)\n",
        "    support_targets, query_targets = targets.chunk(2, dim=0)\n",
        "    return support_imgs, query_imgs, support_targets, query_targets"
      ],
      "metadata": {
        "id": "Ox3wZkR7MUXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, targets = next(iter(val_data_loader))  # We use the validation set since it does not apply augmentations\n",
        "support_imgs, query_imgs, _, _ = split_batch(imgs, targets)\n",
        "support_grid = torchvision.utils.make_grid(support_imgs, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
        "support_grid = support_grid.permute(1, 2, 0)\n",
        "query_grid = torchvision.utils.make_grid(query_imgs, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
        "query_grid = query_grid.permute(1, 2, 0)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n",
        "ax[0].imshow(support_grid)\n",
        "ax[0].set_title(\"Support set\")\n",
        "ax[0].axis('off')\n",
        "ax[1].imshow(query_grid)\n",
        "ax[1].set_title(\"Query set\")\n",
        "ax[1].axis('off')\n",
        "plt.suptitle(\"Few Shot Batch\", weight='bold')\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "U_wp_VpQMUat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_convnet(output_size):\n",
        "    convnet = torchvision.models.DenseNet(growth_rate=32,\n",
        "                                          block_config=(6, 6, 6, 6),\n",
        "                                          bn_size=2,\n",
        "                                          num_init_features=64,\n",
        "                                          num_classes=output_size  # Output dimensionality\n",
        "                                         )\n",
        "    return convnet"
      ],
      "metadata": {
        "id": "hblC-eCSMetE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProtoNet(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, proto_dim, lr):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            proto_dim - Dimensionality of prototype feature space\n",
        "            lr - Learning rate of Adam optimizer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = get_convnet(output_size=self.hparams.proto_dim)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[140, 180], gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_prototypes(features, targets):\n",
        "        # Given a stack of features vectors and labels, return class prototypes\n",
        "        # features - shape [N, proto_dim], targets - shape [N]\n",
        "        classes, _ = torch.unique(targets).sort()  # Determine which classes we have\n",
        "        prototypes = []\n",
        "        for c in classes:\n",
        "            p = features[torch.where(targets == c)[0]].mean(dim=0)  # Average class feature vectors\n",
        "            prototypes.append(p)\n",
        "        prototypes = torch.stack(prototypes, dim=0)\n",
        "        # Return the 'classes' tensor to know which prototype belongs to which class\n",
        "        return prototypes, classes\n",
        "\n",
        "    def classify_feats(self, prototypes, classes, feats, targets):\n",
        "        # Classify new examples with prototypes and return classification error\n",
        "        dist = torch.pow(prototypes[None, :] - feats[:, None], 2).sum(dim=2)  # Squared euclidean distance\n",
        "        preds = F.log_softmax(-dist, dim=1)\n",
        "        labels = (classes[None, :] == targets[:, None]).long().argmax(dim=-1)\n",
        "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
        "        return preds, labels, acc\n",
        "\n",
        "    def calculate_loss(self, batch, mode):\n",
        "        # Determine training loss for a given support and query set\n",
        "        imgs, targets = batch\n",
        "        features = self.model(imgs)  # Encode all images of support and query set\n",
        "        support_feats, query_feats, support_targets, query_targets = split_batch(features, targets)\n",
        "        prototypes, classes = ProtoNet.calculate_prototypes(support_feats, support_targets)\n",
        "        preds, labels, acc = self.classify_feats(prototypes, classes, query_feats, query_targets)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "        self.log(f\"{mode}_loss\", loss)\n",
        "        self.log(f\"{mode}_acc\", acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.calculate_loss(batch, mode=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _ = self.calculate_loss(batch, mode=\"val\")"
      ],
      "metadata": {
        "id": "BBDMGOJ2MewK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "protonet_model = train(ProtoNet,\n",
        "                       device,\n",
        "                        proto_dim=64,\n",
        "                        lr=2e-4,\n",
        "                        train_loader=train_data_loader,\n",
        "                        val_loader=val_data_loader)"
      ],
      "metadata": {
        "id": "mhxmMbJCMeyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "protonet_accuracies = dict()\n",
        "data_feats = None\n",
        "for k in [2, 4, 8, 16, 32]:\n",
        "    protonet_accuracies[k], data_feats = test(protonet_model, test_set, data_feats=data_feats, k_shot=k)\n",
        "    print(f\"Accuracy for k={k}: {100.0*protonet_accuracies[k][0]:4.2f}% (+-{100*protonet_accuracies[k][1]:4.2f}%)\")"
      ],
      "metadata": {
        "id": "7WPTqT1JMe11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_few_shot(acc_dict, name, color=None, ax=None):\n",
        "    sns.set()\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1,1,figsize=(5,3))\n",
        "    ks = sorted(list(acc_dict.keys()))\n",
        "    mean_accs = [acc_dict[k][0] for k in ks]\n",
        "    std_accs = [acc_dict[k][1] for k in ks]\n",
        "    ax.plot(ks, mean_accs, marker='o', markeredgecolor='k', markersize=6, label=name, color=color)\n",
        "    ax.fill_between(ks, [m-s for m,s in zip(mean_accs, std_accs)], [m+s for m,s in zip(mean_accs, std_accs)], alpha=0.2, color=color)\n",
        "    ax.set_xticks(ks)\n",
        "    ax.set_xlim([ks[0]-1, ks[-1]+1])\n",
        "    ax.set_xlabel(\"Number of shots per class\", weight='bold')\n",
        "    ax.set_ylabel(\"Accuracy\", weight='bold')\n",
        "    if len(ax.get_title()) == 0:\n",
        "        ax.set_title(\"Few-Shot Performance \" + name, weight='bold')\n",
        "    else:\n",
        "        ax.set_title(ax.get_title() + \" and \" + name, weight='bold')\n",
        "    ax.legend()\n",
        "    return ax"
      ],
      "metadata": {
        "id": "wnImeJHYMUdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = plot_few_shot(protonet_accuracies, name=\"ProtoNet\", color=\"C1\")\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "-RHW5A6dMz7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}